{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import textstat\n",
    "import time\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebfd5e9d-f55e-47d2-adb9-cac5bade599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaningPipeline:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "     \n",
    "        self.stats = {\n",
    "            'emoticons_removed': 0,\n",
    "            'stop_words_removed': 0,\n",
    "            'special_chars_removed': 0,\n",
    "            'tokens_count_before': 0,\n",
    "            'tokens_count_after': 0,\n",
    "            'lowercase_converted': 0,\n",
    "            'addresses_found': 0,  \n",
    "            'phones_found': 0,\n",
    "            'accounts_found': 0,\n",
    "            'vocab_before': set(),\n",
    "            'vocab_after': set()\n",
    "        }\n",
    "\n",
    "    def clean_and_collect_stats(self, text):\n",
    "        if pd.isna(text) or text == '':\n",
    "            return \"\"\n",
    "\n",
    "        sentences_raw = sent_tokenize(text)\n",
    "        words_raw = word_tokenize(text)\n",
    "        self.stats['tokens_count_before'] += len(words_raw)\n",
    "        self.stats['vocab_before'].update(words_raw)\n",
    "\n",
    "        self.stats['phones_found'] += len(re.findall(r'\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}', text))\n",
    "        self.stats['addresses_found'] += len(re.findall(r'\\d+\\s[A-z]+\\s[St|Ave|Dr|Rd]', text)) \n",
    "        self.stats['accounts_found'] += len(re.findall(r'\\b\\d{8,12}\\b', text)) \n",
    "\n",
    "        emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "        self.stats['emoticons_removed'] += len(emoticons)\n",
    "        \n",
    "        text_no_html = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        special_chars = re.findall(r'[^a-zA-Z0-9\\s]', text_no_html)\n",
    "        self.stats['special_chars_removed'] += len(special_chars)\n",
    "\n",
    "        text_cleaned = re.sub(r'[^a-zA-Z\\s]', '', text_no_html) \n",
    "        \n",
    "        self.stats['lowercase_converted'] += len(re.findall(r'[A-Z]', text_cleaned))\n",
    "        text_lower = text_cleaned.lower()\n",
    "\n",
    "        tokens = word_tokenize(text_lower)\n",
    "        \n",
    "        final_tokens = []\n",
    "        for w in tokens:\n",
    "            if w in self.stop_words:\n",
    "                self.stats['stop_words_removed'] += 1\n",
    "            else:\n",
    "                # --- 6. Step 4: Lemmatization ---\n",
    "                lemma = self.lemmatizer.lemmatize(w)\n",
    "                final_tokens.append(lemma)\n",
    "        \n",
    "        self.stats['tokens_count_after'] += len(final_tokens)\n",
    "        self.stats['vocab_after'].update(final_tokens)\n",
    "\n",
    "        return \" \".join(final_tokens)\n",
    "\n",
    "    def get_lexical_diversity(self, tokens_list):\n",
    "        if len(tokens_list) == 0: return 0\n",
    "        return len(set(tokens_list)) / len(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18c34f5-6928-45d3-8656-d5cf45185dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating raw stats...\n",
      "Processing Cleaning Pipeline... (This might take a while)\n",
      "Calculating final metrics...\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('Sentiment Analysis Dataset.csv', encoding='ISO-8859-1')\n",
    "except FileNotFoundError:\n",
    "    df = pd.read_csv('resouce/Sentiment Analysis Dataset.csv', encoding='ISO-8859-1')\n",
    "\n",
    "if 'SentimentText' in df.columns:\n",
    "    df.rename(columns={'SentimentText': 'review'}, inplace=True)\n",
    "\n",
    "pipeline = TextCleaningPipeline()\n",
    "\n",
    "print(\"Calculating raw stats...\")\n",
    "df['sent_count_raw'] = df['review'].apply(lambda x: len(sent_tokenize(str(x))))\n",
    "df['word_count_raw'] = df['review'].apply(lambda x: len(word_tokenize(str(x))))\n",
    "avg_sent_len_before = (df['word_count_raw'] / df['sent_count_raw']).mean()\n",
    "\n",
    "print(\"Processing Cleaning Pipeline... (This might take a while)\")\n",
    "df['cleaned_review'] = df['review'].apply(pipeline.clean_and_collect_stats)\n",
    "\n",
    "print(\"Calculating final metrics...\")\n",
    "df['readability_score'] = df['cleaned_review'].apply(textstat.flesch_kincaid_grade)\n",
    "df['lexical_diversity'] = df['cleaned_review'].apply(lambda x: pipeline.get_lexical_diversity(x.split()))\n",
    "df['word_count_after'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "all_cleaned_words = \" \".join(df['cleaned_review'].tolist()).split()\n",
    "max_word_len = max([len(w) for w in all_cleaned_words]) if all_cleaned_words else 0\n",
    "\n",
    "end_time = time.time()\n",
    "total_runtime = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be3c554b-9893-470e-be09-7a2cb9266f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################\n",
      "### Text Cleaning Statistics ###\n",
      "##############################\n",
      "- Number of documents: 1048575\n",
      "- Average tokens per document: 17 -> 8\n",
      "- Total vocabulary size: 696811 -> 618111\n",
      "- Average Sentence Length (Raw): 10.90 words/sentence\n",
      "- Max Word Length (Cleaned): 125\n",
      "--------------------\n",
      "- Number of stop words removed: 5550679\n",
      "- Special characters removed: 4480301\n",
      "- Emoticons removed: 10561\n",
      "- Lowercase converted: 3455246\n",
      "--------------------\n",
      "- Addresses found (Approx): 23752\n",
      "- Phone numbers found: 608\n",
      "- Account numbers found: 513\n",
      "--------------------\n",
      "- Average Readability Score (Flesch-Kincaid): 6.83\n",
      "- Average Lexical Diversity: 0.98\n",
      "--------------------\n",
      "- Total runtime: 583.56 seconds (9.73 minutes)\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stats = pipeline.stats\n",
    "\n",
    "print(\"\\n\" + \"#\" * 30)\n",
    "print(\"### Text Cleaning Statistics ###\")\n",
    "print(\"#\" * 30)\n",
    "\n",
    "print(f\"- Number of documents: {len(df)}\")\n",
    "print(f\"- Average tokens per document: {stats['tokens_count_before']/len(df):.0f} -> {stats['tokens_count_after']/len(df):.0f}\")\n",
    "print(f\"- Total vocabulary size: {len(stats['vocab_before'])} -> {len(stats['vocab_after'])}\")\n",
    "print(f\"- Average Sentence Length (Raw): {avg_sent_len_before:.2f} words/sentence\")\n",
    "print(f\"- Max Word Length (Cleaned): {max_word_len}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"- Number of stop words removed: {stats['stop_words_removed']}\")\n",
    "print(f\"- Special characters removed: {stats['special_chars_removed']}\")\n",
    "print(f\"- Emoticons removed: {stats['emoticons_removed']}\")\n",
    "print(f\"- Lowercase converted: {stats['lowercase_converted']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"- Addresses found (Approx): {stats['addresses_found']}\")\n",
    "print(f\"- Phone numbers found: {stats['phones_found']}\")\n",
    "print(f\"- Account numbers found: {stats['accounts_found']}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"- Average Readability Score (Flesch-Kincaid): {df['readability_score'].mean():.2f}\")\n",
    "print(f\"- Average Lexical Diversity: {df['lexical_diversity'].mean():.2f}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"- Total runtime: {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
    "print(\"#\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9b235-16b9-4130-8062-abb3c7cb17c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
